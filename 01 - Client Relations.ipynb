{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Julia Evans' excellent article, [Machine Learning isn't Kaggle Competitions](http://jvns.ca/blog/2014/06/19/machine-learning-isnt-kaggle-competitions/), provides a better overview of this section than I ever could, so I'm just going to include it verbatim:\n",
    "\n",
    "> If you want to predict flight arrival times, what are you really trying to do? Some possible options:\n",
    "\n",
    "> * Help the airline understand which flights are likely to be delayed, so they can fix it.\n",
    "> * Help people buy flights that are less likely to be delayed.\n",
    "> * Warn people if their flight tomorrow is going to be delayed\n",
    "\n",
    "> I've spent time on projects where I didn't understand at all how the model was going to fit into business plans. If this is you, it doesn't matter how good your model is. At all.\n",
    "\n",
    "> Understanding the business problem will also help you decide:\n",
    "\n",
    "> * How accurate does my model really need to be? What kind of false positive rate is acceptable?\n",
    "> * What data can I use? If you're predicting flight days tomorrow, you can look at weather data, but if someone is buying a flight a month from now then you'll have no clue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project scoping\n",
    "This framework for project scoping comes from Max Shron's [talk for the NYC Data Science meetup](https://vimeo.com/98768831).\n",
    "\n",
    "The world gives us vague requests. We have to make things clear before we start or our models will be rambling and unhelpful. The framework consists of four parts:\n",
    "\n",
    "1. Context\n",
    "2. Need\n",
    "3. Vision\n",
    "4. Outcome\n",
    "\n",
    "**Example of a bad scope:** \n",
    "\n",
    "We're working with a company that has a subscription business. \n",
    "\n",
    "> CEO: It would be great if you could help us build a churn model.\n",
    "\n",
    "> Us: Alright - we're going to use a logistic regression to predict when someone is about to stop using the product.\n",
    "\n",
    "Why is this a bad scope? Well, it's not actionable for the company, and includes irrelevant details.\n",
    "\n",
    "## Context\n",
    "Who are we working with? What are the big picture, long-term goals?\n",
    "\n",
    "> \"The company has a subscription model. Their goal is to improve profitability.\"\n",
    "\n",
    "This is different than a churn model for someone who's just trying to get as many users as possible. It's different than a churn model where someone is just trying to increase revenue as much as possible.\n",
    "\n",
    "## Need\n",
    "What is the particular knowledge we are missing?\n",
    "\n",
    "> \"We want to understand who drops off **early enough** that we can intervene.\"\n",
    "\n",
    "Finding out two seconds before someone is about to quit is not that helpful.\n",
    "\n",
    "## Vision\n",
    "What would it look like to solve the problem?\n",
    "\n",
    "> \"We will build a predictive model **using behavioral data** to predict who will drop off - early enough to be useful.\"\n",
    "\n",
    "Data sources matter. The kinds of intervention possible matter (different messaging channels, different content types).\n",
    "\n",
    "## Outcome\n",
    "Who will be responsible for next steps? How will we know if we are correct?\n",
    "\n",
    "> \"The tech team will implement the model in a batch process to run daily, automatically sending out email offers. We will calculate success metrics (precision and recall) on held out users, and send a weekly email of stats to stay on top of outcomes.\"\n",
    "\n",
    "Cross-validation isn't actually success. We need a control group.\n",
    "\n",
    "** Example of a better scope: ** \n",
    "\n",
    "**Context:** We are working with a hospital system that has had 250k patients in the last 20 years. The CEO is interested in building a tool for reducing medical issues.\n",
    "\n",
    "**Need:** After talking to some doctors, some belief that there is an overuse of antibiotics - but this is hard to detect.\n",
    "\n",
    "**Vision:** A pilot investigation. If we find a signal, we will build a repeatable flagging tool.\n",
    "\n",
    "**Outcome:** The CMO will decide if the pilot is valuable based on our report. The automated tool would be run by the CMO on-demand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client communication\n",
    "Interviews are obviously the most straightforward way to get this information - but tools from the design industry like roleplaying and storytelling can provide additional context.\n",
    "\n",
    "Before ever getting started, it helps to sketch out ideas of what the end visualizations could look like. \n",
    "\n",
    "Or put it in a sentence - the kind of insight we want to be able to deliver once we've done the work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client best practices\n",
    "Be able to advise clients on steps they can take to improve their data for future analysis. Sasha Laundry laid out the steps that non-technical clients should take to audit their data at her [PyData NYC 2014 talk](http://blog.sashalaundy.com/talks/data-audit/), which I've summarized below:\n",
    "\n",
    "## Data completeness\n",
    "Are business-critical fields okay? Check volume (like comparing sessions from Google Analytics vs internal system logs). \n",
    "\n",
    "Engineers working without marketers have a tendency to log errors but not other potentially meaningful measures of user behavior. Simple things - if you have five categories, make sure you’re not just tracking two of them.\n",
    "\n",
    "## Data correctness\n",
    "Again, check against existing knowledge. In most cases, the data shouldn’t shock the client. \n",
    "\n",
    "Speed up early data analysis by using [csvkit](http://csvkit.readthedocs.org/) - a command line tool for working with CSV files. \n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: state\n",
      "  2: county\n",
      "  3: fips\n",
      "  4: nsn\n",
      "  5: item_name\n",
      "  6: quantity\n",
      "  7: ui\n",
      "  8: acquisition_cost\n",
      "  9: total_cost\n",
      " 10: ship_date\n",
      " 11: federal_supply_category\n",
      " 12: federal_supply_category_name\n",
      " 13: federal_supply_class\n",
      " 14: federal_supply_class_name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100   150  100   150    0     0    151      0 --:--:-- --:--:-- --:--:--   151\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\r",
      "100   161  100   161    0     0    136      0  0:00:01  0:00:01 --:--:--  4472\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\r",
      "100 65331  100 65331    0     0  27691      0  0:00:02  0:00:02 --:--:--  439k\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Start by creating a clean workspace\n",
    "mkdir csvkit_tutorial\n",
    "cd csvkit_tutorial\n",
    "\n",
    "# Fetch the data\n",
    "curl -L -O https://github.com/onyxfish/csvkit/raw/master/examples/realdata/ne_1033_data.xlsx\n",
    "\n",
    "# Copy the contents of the Excel file to a new CSV\n",
    "in2csv ne_1033_data.xlsx > data.csv\n",
    "\n",
    "# Log headers from data.csv\n",
    "csvcut -n data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "county,item_name,quantity\n",
      "ADAMS,\"RIFLE,7.62 MILLIMETER\",1\n",
      "ADAMS,\"RIFLE,7.62 MILLIMETER\",1\n",
      "ADAMS,\"RIFLE,7.62 MILLIMETER\",1\n",
      "ADAMS,\"RIFLE,7.62 MILLIMETER\",1\n",
      "ADAMS,\"RIFLE,7.62 MILLIMETER\",1\n",
      "ADAMS,\"RIFLE,7.62 MILLIMETER\",1\n",
      "BUFFALO,\"RIFLE,5.56 MILLIMETER\",1\n",
      "BUFFALO,\"RIFLE,5.56 MILLIMETER\",1\n",
      "BUFFALO,\"RIFLE,5.56 MILLIMETER\",1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd csvkit_tutorial\n",
    "\n",
    "# Look at the data in just a few of our columns \n",
    "# csvcut -c 2,5,6 data.csv | head\n",
    "\n",
    "# We can do this by index or by column name\n",
    "csvcut -c county,item_name,quantity data.csv | head "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data connectability\n",
    "Can the data be joined easily? Not just to other client datasets - if, for example, the client team has weird internal definitions of territories that don't match up with zip codes or longitude/latitude, you’re going to have a hell of a time getting insights from any other source.\n",
    "\n",
    "The best way to do this is to actually perform a join, and check it against existing knowledge.\n",
    "\n",
    "Make sure the client understands the importance of unique IDs that are persistent across all databases - marketing, in-app, transactional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "## Video\n",
    "1. Thinking with Data, Max Shron (NYC Data Science Meetup 2014) - [https://vimeo.com/98768831](https://vimeo.com/98768831)\n",
    "2. How to Make Your Future Data Scientists Love You, Sasha Laundy (PyData NYC 2014) - [http://blog.sashalaundy.com/talks/data-audit/](http://blog.sashalaundy.com/talks/data-audit/)\n",
    "\n",
    "## Tools\n",
    "1. csvkit - [http://csvkit.readthedocs.io/](http://csvkit.readthedocs.io/)\n",
    "2. data_hacks - [https://github.com/bitly/data_hacks](https://github.com/bitly/data_hacks)\n",
    "\n",
    "## Reading\n",
    "### Articles\n",
    "1. Machine Learning isn't Kaggle Competitions, Julia Evans - [http://jvns.ca/blog/2014/06/19/machine-learning-isnt-kaggle-competitions/](http://jvns.ca/blog/2014/06/19/machine-learning-isnt-kaggle-competitions/)\n",
    "\n",
    "### Books\n",
    "1. Thinking with Data, Max Shron - [http://shop.oreilly.com/product/0636920029182.do](http://shop.oreilly.com/product/0636920029182.do)\n",
    "2. Data Science at the Command Line, Jeroen Janssens - [http://shop.oreilly.com/product/0636920032823.do](http://shop.oreilly.com/product/0636920032823.do)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
